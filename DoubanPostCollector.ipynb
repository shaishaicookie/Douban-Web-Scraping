{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd4cc50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas\n",
    "import os\n",
    "import pdfkit\n",
    "import imgkit\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae757cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy = [{\"http\":\"103.37.141.69:80\"},\n",
    "{\"http\":\"14.215.212.37:9168\"},\n",
    "{\"http\":\"111.59.199.58:8188\"},\n",
    "{\"http\":\"14.215.212.37:9168\"},\n",
    "{\"http\":\"223.82.60.202:8060\"},\n",
    "{\"http\":\"222.78.6.190:8083\"},\n",
    "{\"http\":\"106.15.197.250:8001\"},\n",
    "{\"http\":\"222.66.202.6:80\"},\n",
    "{\"http\":\"101.34.214.152:8001\"},\n",
    "{\"http\":\"14.215.212.37:9168\"},\n",
    "{\"http\":\"202.55.5.209:8090\"},\n",
    "{\"http\":\"111.3.111.179:30001\"},\n",
    "{\"http\":\"124.93.201.59:42672\"},\n",
    "{\"http\":\"106.15.197.250:8001\"},\n",
    "{\"http\":\"111.59.199.58:8188\"},\n",
    "{\"http\":\"106.15.197.250:8001\"},\n",
    "{\"http\":\"115.218.5.177:9000\"},\n",
    "{\"http\":\"117.114.149.66:55443\"},\n",
    "{\"http\":\"47.92.234.75:80\"},\n",
    "{\"http\":\"112.14.47.6:52024\"}]\n",
    "\n",
    "headers={'User-agent': 'Mozilla/5.0'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b56cd153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findElitePostPageNumber(groupID):\n",
    "    startURL = \"https://www.douban.com/group/\" + str(groupID) + \"/discussion?start=0&type=elite\"\n",
    "    r = requests.get(startURL, headers=headers, proxies=random.choice(proxy))\n",
    "    c = r.content\n",
    "    soup = BeautifulSoup(c, \"html.parser\")\n",
    "    pageNumber = int(soup.find(\"span\", {\"class\":\"thispage\"}).attrs[\"data-total-page\"])\n",
    "    return pageNumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "163d6e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findElitePostURLs(groupID):\n",
    "    ElitePostURLs = []\n",
    "    baseURL = \"https://www.douban.com/group/\" + str(groupID) + \"/discussion?start=\"\n",
    "    pageNumber = findElitePostPageNumber(groupID)\n",
    "    for page in range(0, pageNumber*25, 25):\n",
    "        currentPageURL = baseURL + str(page) + \"&type=elite\"\n",
    "        # check process\n",
    "        print(page)\n",
    "        r = requests.get(currentPageURL, headers=headers, proxies=random.choice(proxy))\n",
    "        c = r.content\n",
    "        soup = BeautifulSoup(c, \"html.parser\")\n",
    "        all = soup.find_all(\"td\", {\"class\":\"title\"})\n",
    "        for item in all:\n",
    "            postInfo = {}\n",
    "            # find post url\n",
    "            elitePostURL = item.find(\"a\").attrs[\"href\"]\n",
    "            postInfo[\"postURL\"] = elitePostURL\n",
    "            # find post title                                 \n",
    "            postInfo[\"postTitle\"] = item.find(\"a\").attrs[\"title\"].replace(' ', '-').replace('/', '')\n",
    "\n",
    "            ElitePostURLs.append(postInfo)\n",
    "            \n",
    "    df = pandas.DataFrame(ElitePostURLs)\n",
    "    df = df.drop_duplicates(\"postURL\")\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b689c5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "basePath = '/Users/sw/Desktop/DoubanSpider'\n",
    "def createOutputDir(groupID):\n",
    "    groupPath = os.path.join(basePath, \"Group\"+str(groupID))\n",
    "    os.mkdir(groupPath)\n",
    "    groupURLs = findElitePostURLs(groupID)\n",
    "    for i in range(len(groupURLs)):\n",
    "        postTitle = groupURLs[\"postTitle\"][i]\n",
    "        postPath = os.path.join(groupPath, postTitle)\n",
    "        os.mkdir(postPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b858e840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def savePostInsideLinks(groupID, postURL, postTitle):\n",
    "    r = requests.get(postURL, headers=headers, proxies=random.choice(proxy))\n",
    "    c = r.content\n",
    "    soup = BeautifulSoup(c, \"html.parser\")\n",
    "\n",
    "    links = soup.find_all(\"a\", {\"class\":\"link\"})\n",
    "    # 确认有没有链接\n",
    "    if len(links):\n",
    "        postInsideLinks = []\n",
    "        for element in links:\n",
    "            insideLinkInfo = {}\n",
    "            insideLinkTitle = element.text\n",
    "            insideLinkInfo[\"linkTitle\"] = insideLinkTitle\n",
    "            insidelink = element.attrs[\"href\"]\n",
    "            insideLinkInfo[\"linkAddress\"] = insidelink\n",
    "            postInsideLinks.append(insideLinkInfo)\n",
    "        postInsideLinks = pandas.DataFrame(postInsideLinks)\n",
    "    \n",
    "        postPath = basePath + \"/Group\"+str(groupID)+\"/\" + postTitle+\"/\"\n",
    "\n",
    "        # csv format\n",
    "        postInsideLinks.to_csv(postPath+\"link\"+\".csv\", encoding=\"utf-8\")\n",
    "        # txt format\n",
    "        postInsideLinks.to_csv(postPath+\"link\"+\".txt\", sep='\\t', index=False, encoding=\"utf-8\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec1b0fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postImageGifDownloader(groupID, postURL, postTitle):\n",
    "    postPath = basePath + \"/Group\"+str(groupID)+\"/\" + postTitle+\"/\"\n",
    "\n",
    "    r = requests.get(postURL, headers=headers, proxies=random.choice(proxy))\n",
    "    c = r.content\n",
    "    soup = BeautifulSoup(c, \"html.parser\")\n",
    "\n",
    "    images = soup.find_all(\"div\", {\"class\":\"image-wrapper\"})\n",
    "    # check whether exist images in the post\n",
    "    if len(images):\n",
    "        for i in range(len(images)):\n",
    "            try:\n",
    "                # is a gif\n",
    "                imageLink = images[i].find(\"img\").attrs[\"data-original-url\"]\n",
    "                imageName = '{}-{}.gif'.format(postTitle, i)\n",
    "            except:\n",
    "                # is a jpg\n",
    "                imageLink = images[i].find(\"img\").attrs[\"src\"]\n",
    "                imageName = '{}-{}.jpg'.format(postTitle, i)    \n",
    "            with open(postPath+imageName, \"wb\") as f:\n",
    "                postImage = requests.get(imageLink, headers=headers, proxies=random.choice(proxy)).content\n",
    "                f.write(postImage)\n",
    "                print('Writing', imageName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30116c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def savePostContentText(groupID, postURL, postTitle):\n",
    "    r = requests.get(postURL, headers=headers, proxies=random.choice(proxy))\n",
    "    c = r.content\n",
    "    soup = BeautifulSoup(c, \"html.parser\")\n",
    "\n",
    "    contentTexts = soup.find_all(\"p\", class_=False, id=False, alignment=\"\")\n",
    "    result = []\n",
    "    for line in contentTexts:\n",
    "        result.append(line.text.replace(\"\\n\", \"\"))\n",
    "    result = pandas.DataFrame(result)\n",
    "\n",
    "    basePath = '/Users/sw/Desktop/DoubanSpider'\n",
    "    postPath = basePath + \"/Group\"+str(groupID)+\"/\" + postTitle+\"/\"\n",
    "    result.to_csv(postPath+\"正文\"+\".txt\", sep='\\t',index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a111902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def savePostComments(groupID, postURL, postTitle):\n",
    "    r = requests.get(postURL, headers=headers, proxies=random.choice(proxy))\n",
    "    c = r.content\n",
    "    soup = BeautifulSoup(c, \"html.parser\")\n",
    "    \n",
    "    comments = soup.find_all(\"div\", {\"class\":\"reply-doc content\"})\n",
    "    postComments = []\n",
    "    for i in range(len(comments)):\n",
    "        commentInfo = {}\n",
    "        commentUser = comments[i].find(\"a\").text\n",
    "        commentContent = comments[i].find(\"p\").text\n",
    "        commentInfo[\"user\"] = commentUser\n",
    "        commentInfo[\"content\"] = commentContent\n",
    "        postComments.append(commentInfo)\n",
    "    \n",
    "    postComments = pandas.DataFrame(postComments)\n",
    "\n",
    "    postPath = basePath + \"/Group\"+str(groupID)+\"/\" + postTitle+\"/\"\n",
    "    # csv format\n",
    "    postComments.to_csv(postPath+\"comment\"+\".csv\", encoding=\"utf-8\")\n",
    "    # txt format\n",
    "    postComments.to_csv(postPath+\"comment\"+\".txt\",sep='\\t',index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b34c226d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def savePostAsPdf(groupID, postURL, postTitle):\n",
    "    postPath = basePath + \"/Group\"+str(groupID)+\"/\" + postTitle+\"/\"\n",
    "    pdfoptions = {'dpi':400}\n",
    "    pdfkit.from_url(postURL, postPath+postTitle+\"-帖子全部\"+\".pdf\", options=pdfoptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f209fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def groupElitePostCollector(groupID):\n",
    "    # 找到豆瓣小组所有精华帖子的URL和标题名\n",
    "    groupElitePostInfos = findElitePostURLs(groupID)\n",
    "\n",
    "    # 创建路径 方便数据整理\n",
    "    createOutputDir(groupID)\n",
    "    for i in range(len(groupElitePostInfos)):\n",
    "        postURL = groupElitePostInfos[\"postURL\"][i]\n",
    "        postTitle = groupElitePostInfos[\"postTitle\"][i]\n",
    "\n",
    "        # 1. 储存精华帖中的链接和链接标题到 csv txt\n",
    "        savePostInsideLinks(groupID, postURL, postTitle)\n",
    "\n",
    "        # 2. 储存精华帖中的所有图片或者动图，按照出现顺序标号命名\n",
    "        postImageGifDownloader(groupID, postURL, postTitle)\n",
    "\n",
    "        # 3. 储存精华帖中的所有正文信息\n",
    "        savePostContentText(groupID, postURL, postTitle)\n",
    "\n",
    "        # 4. 储存精华帖中所有评论内容 评论ID 评论内容 到csv txt\n",
    "        savePostComments(groupID, postURL, postTitle)\n",
    "\n",
    "        # 5. 把精华帖生成pdf文件到指定路径\n",
    "        savePostAsPdf(groupID, postURL, postTitle)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9b5ff723cd38cf7d359aa2bf5fe7978a9ff73d1f62f5b88243d6c54060f370c5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
