{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import time \n",
    "import requests\n",
    "import pandas       \n",
    "from bs4 import BeautifulSoup     \n",
    "from selenium import webdriver                                                                                                                                                      \n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3549f6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "loginURL = \"https://accounts.douban.com/passport/login\"\n",
    "driver.get(loginURL)\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b0e33a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findElitePostPageNumber(groupID):\n",
    "    startURL = \"https://www.douban.com/group/\" + str(groupID) + \"/discussion?start=0&type=elite\"\n",
    "    driver.get(startURL)\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, features='lxml')\n",
    "    pageNumber = int(soup.find(\"span\", {\"class\":\"thispage\"}).attrs[\"data-total-page\"])\n",
    "    return pageNumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2f96650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findElitePostURLs(groupID):\n",
    "    ElitePostURLs = []\n",
    "    baseURL = \"https://www.douban.com/group/\" + str(groupID) + \"/discussion?start=\"\n",
    "    pageNumber = findElitePostPageNumber(groupID)\n",
    "    for page in range(0, pageNumber*25, 25):\n",
    "        currentPageURL = baseURL + str(page) + \"&type=elite\"\n",
    "        driver.get(currentPageURL)\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, features='lxml')\n",
    "        all = soup.find_all(\"td\", {\"class\":\"title\"})\n",
    "        for item in all:\n",
    "            postInfo = {}\n",
    "            # find post url\n",
    "            elitePostURL = item.find(\"a\").attrs[\"href\"]\n",
    "            postInfo[\"postURL\"] = elitePostURL\n",
    "            # find post title                                 \n",
    "            postInfo[\"postTitle\"] = item.find(\"a\").attrs[\"title\"].replace(' ', '-').replace('/', '')\n",
    "\n",
    "            ElitePostURLs.append(postInfo)\n",
    "            \n",
    "    df = pandas.DataFrame(ElitePostURLs)\n",
    "    df = df.drop_duplicates(\"postURL\")\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88a46e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "basePath = '/Users/sw/Desktop/DoubanSpider'\n",
    "def createOutputDir(groupID):\n",
    "    groupPath = os.path.join(basePath, \"Group\"+str(groupID))\n",
    "    os.mkdir(groupPath)\n",
    "    groupURLs = findElitePostURLs(groupID)\n",
    "    for i in range(len(groupURLs)):\n",
    "        postTitle = groupURLs[\"postTitle\"][i]\n",
    "        postPath = os.path.join(groupPath, postTitle)\n",
    "        os.mkdir(postPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80932851",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postImageGifDownloader(groupID, postURL, postTitle):\n",
    "    postPath = basePath + \"/Group\"+str(groupID)+\"/\" + postTitle+\"/\"\n",
    "    driver.get(postURL)\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, features='lxml')\n",
    "    images = soup.find_all(\"div\", {\"class\":\"image-wrapper\"})\n",
    "    # check whether exist images in the post\n",
    "    if len(images):\n",
    "        for i in range(len(images)):\n",
    "            try:\n",
    "                # is a gif\n",
    "                imageLink = images[i].find(\"img\").attrs[\"data-original-url\"]\n",
    "                imageName = '{}-{}.gif'.format(postTitle, i)\n",
    "            except:\n",
    "                # is a jpg\n",
    "                imageLink = images[i].find(\"img\").attrs[\"src\"]\n",
    "                imageName = '{}-{}.jpg'.format(postTitle, i)    \n",
    "            with open(postPath+imageName, \"wb\") as f:\n",
    "                postImage = requests.get(imageLink).content\n",
    "                f.write(postImage)\n",
    "                print('Writing', imageName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91a2b885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def savePostInsideLinks(groupID, postURL, postTitle):\n",
    "    postPath = basePath + \"/Group\"+str(groupID)+\"/\" + postTitle+\"/\"\n",
    "    driver.get(postURL)\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, features='lxml')\n",
    "\n",
    "    links = soup.find_all(\"a\", {\"class\":\"link\"})\n",
    "    # 确认有没有链接\n",
    "    if len(links):\n",
    "        postInsideLinks = []\n",
    "        for element in links:\n",
    "            insideLinkInfo = {}\n",
    "            insideLinkTitle = element.text\n",
    "            insideLinkInfo[\"linkTitle\"] = insideLinkTitle\n",
    "            insidelink = element.attrs[\"href\"]\n",
    "            insideLinkInfo[\"linkAddress\"] = insidelink\n",
    "            postInsideLinks.append(insideLinkInfo)\n",
    "        postInsideLinks = pandas.DataFrame(postInsideLinks)\n",
    "    \n",
    "        # csv format\n",
    "        postInsideLinks.to_csv(postPath+\"link\"+\".csv\", encoding=\"utf-8\")\n",
    "        # txt format\n",
    "        postInsideLinks.to_csv(postPath+\"link\"+\".txt\", sep='\\t', index=False, encoding=\"utf-8\")\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4585bacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def savePostContentText(groupID, postURL, postTitle):\n",
    "    driver.get(postURL)\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, features='lxml')\n",
    "    contentTexts = soup.find_all(\"p\", class_=False, id=False, alignment=\"\")\n",
    "    result = []\n",
    "    for line in contentTexts:\n",
    "        result.append(line.text.replace(\"\\n\", \"\"))\n",
    "    result = pandas.DataFrame(result)\n",
    "\n",
    "    basePath = '/Users/sw/Desktop/DoubanSpider'\n",
    "    postPath = basePath + \"/Group\"+str(groupID)+\"/\" + postTitle+\"/\"\n",
    "    result.to_csv(postPath+\"正文\"+\".txt\", sep='\\t',index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4aba2340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def savePostEntireScreenshot(postURL, postTitle):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.headless = True\n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install(), options=options)\n",
    "    driver.implicitly_wait(100)\n",
    "\n",
    "    postScreenShotName = postTitle + \".png\"\n",
    "\n",
    "    driver.get(postURL)\n",
    "\n",
    "    S = lambda X: driver.execute_script('return document.body.parentNode.scroll'+X)\n",
    "    driver.set_window_size(S('Width'),S('Height'))                                                                                                              \n",
    "    driver.find_element_by_class_name('article').screenshot(postScreenShotName)\n",
    "    print('Writing', postScreenShotName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3d101d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elitePostDownloader(groupID):\n",
    "    elitePostInfos = findElitePostURLs(groupID)\n",
    "    createOutputDir(groupID)\n",
    "    for i in range(len(elitePostInfos)):\n",
    "        postURL = elitePostInfos[\"postURL\"][i]\n",
    "        postTitle = elitePostInfos[\"postTitle\"][i]\n",
    "        # 1. 图片 Gif\n",
    "        postImageGifDownloader(groupID, postURL, postTitle)\n",
    "        # 2. 正文内容\n",
    "        savePostContentText(groupID, postURL, postTitle)\n",
    "        # 3. 正文链接\n",
    "        savePostInsideLinks(groupID, postURL, postTitle)\n",
    "        # 4. 全部帖子截图\n",
    "        savePostEntireScreenshot(postURL, postTitle)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9b5ff723cd38cf7d359aa2bf5fe7978a9ff73d1f62f5b88243d6c54060f370c5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
